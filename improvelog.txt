Gmk0
NN Policy
 input 15*15*2
 conv1 3*3*2*16 SAME relu
 conv2 3*3*16*48 SAME relu
 conv3 3*3*48*96 SAME relu
 conv4 3*3*96*96 SAME relu
 conv5 3*3*96*96 SAME relu
 conv6 3*3*96*96 SAME relu
 conv7 3*3*96*96 SAME relu
 conv8 3*3*96*1 SAME relu
 fc1 reshape conv8 15*15
 y softmax fc1
Loss cross_enptroy
NN Value
 input 15*15*2
 conv1 3*3*2*16 SAME relu
 conv2 3*3*16*48 SAME relu
 conv3 3*3*48*96 SAME relu
 conv4 3*3*96*96 SAME relu
 conv5 3*3*96*96 SAME relu
 conv6 3*3*96*96 SAME relu
 conv7 3*3*96*96 SAME relu
 conv8 3*3*96*1 SAME relu
 fc1 reshape conv8 225 relu
 y matmul fc1
Loss avg_sqr

GameRecord
  int stepcount
  int[stepcount] move form 0 to 225
  int winner 0,1,2

GameTrainData
  int stepcount
  int[stepcount] move
  pos
    prob, move
  pos policy[stepcount][225]   sigma policy=1
  int winner for z
  
mini_batch
  int count
  GameTrainData[count]

12-30
  target: conv->res
  +bn, +dual 

1-22
  starting SL test
  problem: speed of mcts too slow

1-23
  SL test ok
  completed NN estimate
  generate data test
  optimized mcts selection, 2-3x speed up

1-24
  self dataset ok, but found a few problem
  add dirlect noise
  start iteration 0

1-25
  found small fault at iteration 0
  output of winner reversed(correct by retransdata)
  a small fault in SL data transform and a big fault in It0 data trans.(fixed)

1-26
  iteration 0 ok
  test shows decay_reward is important (decay=0.85)
  weight_0 vs random
w    41         9
b    44         6
85% PASS

1-27
  optimzed tree search, 3x speed up

1-28
  training I1, decay=0.88 le=0.05 step=16k
  weight_1 vs weight_0
w    29         21
b    32         18
61% PASS
  tringing I2, decay=0.88 le=0.05 step=16k

1-29
  weight_2 vs weight_1
w    33         17
b    32         18
65% PASS
  training I3, decay=0.88 le=0.05 step=16k
weight_3 vs weight_2
w    30         20
b    29         21
59% PASS
weight_3 vs weight_1
w    37         13
b    37         13
extra test ok